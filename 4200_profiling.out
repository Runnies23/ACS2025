Switching to group 'render' and re-executing...
True
Checking GPU Availability...
Torch version: 2.10.0+rocm7.1
Is ROCm/CUDA available?: True
Device count: 1
-------------------------------------------
Processing dataset: 
2026-02-17 09:44:50.902054: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-02-17 09:44:50.933005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-02-17 09:44:50.951614: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-02-17 09:44:50.963354: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-02-17 09:44:50.983301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-02-17 09:44:52.261131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[rank: 0] Global seed set to 123
Nvidia GPU detected!
Total visible GPUs: 1
--- GPU 0 ---
Name: AMD Radeon PRO W7900D
Total memory: 49136 MB
it's find only 1 gpus can't do multi gpu
still run it's any ways
INFO:root:[INFO] Start Inference
INFO:root:init valueable: 0.005006s
/mnt/ASC1664/miniconda3/envs/unifolm-wma_run/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
INFO:mainlogger:LatentVisualDiffusion: Running in v-prediction mode
Force INTO XFORMERS IS AVAILBLE
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
INFO:unifolm_wma_run.models.diffusion_head.conditional_unet1d:number of parameters: 5.010531e+08
INFO:unifolm_wma_run.models.diffusion_head.conditional_unet1d:number of parameters: 5.010531e+08
INFO:root:[INFO] this is the model architecture
INFO:root:[INFO] input_blocks
INFO:root:ModuleList(
  (0): TimestepEmbedSequential(
    (0): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (1-2): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=320, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=1024, out_features=320, bias=False)
            (to_v): Linear(in_features=1024, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=320, bias=False)
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
  )
  (3): TimestepEmbedSequential(
    (0): Downsample(
      (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (4): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (5): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (6): TimestepEmbedSequential(
    (0): Downsample(
      (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (7): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
  (8): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
  (9): TimestepEmbedSequential(
    (0): Downsample(
      (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (10-11): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
  )
)
INFO:root:[INFO] output_blocks
INFO:root:ModuleList(
  (0-1): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
  )
  (2): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): Upsample(
      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (3-4): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
  (5): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (3): Upsample(
      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (6): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (7): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (8): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (3): Upsample(
      (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (9): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=320, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=1024, out_features=320, bias=False)
            (to_v): Linear(in_features=1024, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=320, bias=False)
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
  )
  (10-11): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=320, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=1024, out_features=320, bias=False)
            (to_v): Linear(in_features=1024, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=320, bias=False)
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
  )
)
INFO:root:[INFO] action_unet
INFO:root:ConditionalUnet1D(
  (obs_encoder): MultiImageObsEncoder(
    (key_model_map): ModuleDict(
      (image): Sequential(
        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (1): GroupNorm(4, 64, eps=1e-05, affine=True)
        (2): ReLU(inplace=True)
        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (4): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)
          )
          (1): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)
          )
        )
        (5): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)
            (downsample): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): GroupNorm(8, 128, eps=1e-05, affine=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)
          )
        )
        (6): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)
            (downsample): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): GroupNorm(16, 256, eps=1e-05, affine=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)
          )
        )
        (7): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): GroupNorm(32, 512, eps=1e-05, affine=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)
          )
        )
      )
    )
    (key_transform_map): ModuleDict(
      (image): Sequential(
        (0): Identity()
        (1): Identity()
        (2): Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      )
    )
    (spatial_softmax): SpatialSoftmax(num_kp=128, temperature=1.0, noise=0.0)
  )
  (mid_modules): ModuleList(
    (0-1): 2 x ConditionalResidualBlock1D(
      (blocks): ModuleList(
        (0-1): 2 x Conv1dBlock(
          (block): Sequential(
            (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
            (2): Mish()
          )
        )
      )
      (cond_encoder): Sequential(
        (0): Mish()
        (1): Linear(in_features=3232, out_features=4096, bias=True)
      )
      (residual_conv): Identity()
    )
    (2): Identity()
  )
  (spatial_softmax_blocks): ModuleList(
    (0): SpatialSoftmax(num_kp=320, temperature=1.0, noise=0.0)
    (1): SpatialSoftmax(num_kp=640, temperature=1.0, noise=0.0)
    (2-6): 5 x SpatialSoftmax(num_kp=1280, temperature=1.0, noise=0.0)
    (7): SpatialSoftmax(num_kp=640, temperature=1.0, noise=0.0)
    (8): SpatialSoftmax(num_kp=320, temperature=1.0, noise=0.0)
  )
  (diffusion_step_encoder): Sequential(
    (0): SinusoidalPosEmb()
    (1): Linear(in_features=128, out_features=512, bias=True)
    (2): Mish()
    (3): Linear(in_features=512, out_features=128, bias=True)
  )
  (up_modules): ModuleList(
    (0): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(4096, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Upsample1d(
        (conv): ConvTranspose1d(2048, 2048, kernel_size=(4,), stride=(2,), padding=(1,))
      )
    )
    (1): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(3072, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Conv1d(3072, 1024, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Upsample1d(
        (conv): ConvTranspose1d(1024, 1024, kernel_size=(4,), stride=(2,), padding=(1,))
      )
    )
    (2): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1536, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Upsample1d(
        (conv): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))
      )
    )
    (3): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(768, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Identity()
      )
      (2-3): 2 x Identity()
    )
  )
  (down_modules): ModuleList(
    (0): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(16, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Conv1d(16, 256, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Downsample1d(
        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (1): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Downsample1d(
        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (2): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Downsample1d(
        (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (3): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Identity()
      )
      (2-3): 2 x Identity()
    )
  )
  (final_conv): Sequential(
    (0): Conv1dBlock(
      (block): Sequential(
        (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): GroupNorm(8, 256, eps=1e-05, affine=True)
        (2): Mish()
      )
    )
    (1): Conv1d(256, 16, kernel_size=(1,), stride=(1,))
  )
  (proj_in_action): Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (proj_in_horizon): Sequential(
    (0): Linear(in_features=16, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (proj_out_action): Sequential(
    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=32, out_features=1, bias=True)
  )
  (proj_out_horizon): Sequential(
    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=32, out_features=16, bias=True)
  )
)
INFO:root:[INFO] Done load WMAmodel
AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
INFO:root:Loaded ViT-H-14 model config.
INFO:root:Loaded ViT-H-14 model config.
/mnt/ASC1664/unifolm-wma-0-dual/ASC26-Embodied-World-Model-Optimization/ckpts/unifolm_wma_dual.ckpt
INFO:root:load models: 25.786966s
>>> model checkpoint loaded.
>>> Load pre-trained model ...
INFO:root:load model checkpoint: 20.281248s
INFO:root:***** Configing Data *****
>>> unitree_z1_stackbox: 1 data samples loaded.
>>> unitree_z1_stackbox: data stats loaded.
>>> unitree_z1_stackbox: normalizer initiated.
>>> unitree_z1_dual_arm_stackbox: 1 data samples loaded.
>>> unitree_z1_dual_arm_stackbox: data stats loaded.
>>> unitree_z1_dual_arm_stackbox: normalizer initiated.
>>> unitree_z1_dual_arm_stackbox_v2: 1 data samples loaded.
>>> unitree_z1_dual_arm_stackbox_v2: data stats loaded.
>>> unitree_z1_dual_arm_stackbox_v2: normalizer initiated.
>>> unitree_z1_dual_arm_cleanup_pencils: 1 data samples loaded.
>>> unitree_z1_dual_arm_cleanup_pencils: data stats loaded.
>>> unitree_z1_dual_arm_cleanup_pencils: normalizer initiated.
>>> unitree_g1_pack_camera: 1 data samples loaded.
>>> unitree_g1_pack_camera: data stats loaded.
>>> unitree_g1_pack_camera: normalizer initiated.
>>> Dataset is successfully loaded ...
INFO:root:load data: 0.126505s
>>> Generate 16 frames under each generation ...
INFO:root:set up data: 0.000025s
INFO:root:[INFO] Start running Inference
INFO:root:get_init_frame_path: 0.000042s
INFO:root:mkdir: 0.000112s
INFO:root:get_transition_path: 0.000015s
DEBUG:h5py._conv:Creating converter from 3 to 5
INFO:root:load file h5py: 0.007041s
INFO:root:[INFO] start running on loop frame_stride
INFO:root:mkdir => save dir: 0.000069s
INFO:root:init value: 0.000008s
DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13
DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 41 9
DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 62 4096
INFO:root:PIL open image : 0.014671s
INFO:root:create action & state: 0.000405s
INFO:root:normalize action - state: 0.001085s
INFO:root:spatial_transform: 0.001569s
INFO:root:prepare_init_input : 0.035582s
INFO:root:init observation: 0.007519s
INFO:root:init condition input w/ populate_queues: 0.000025s
INFO:root:[INFO] start running on loop n_iter
INFO:root:[INFO] start 1/12 n_iter (start with idx:1)
INFO:root:get decision model observation: 0.018759s
>>> Step 1: generating actions ...
INFO:root:[INFO] Step 1: generating actions ...
INFO:root:[INFO] IN image_guided_synthesis_sim_mode function
INFO:root:init model & noise: 0.000342s
INFO:root:permute image: 0.000015s
INFO:root:rearrange image: 0.001234s
INFO:root:CLIP Embedded Image: 0.650213s
INFO:root:Projector Image model: 0.151444s
INFO:root:[INFO] get_latent_z function
INFO:root:get_latent_z => rearrange video: 0.000116s
INFO:root:get_latent_z => encode_first_stage: 0.264913s
INFO:root:get_latent_z => rearrange z: 0.000129s
INFO:root:conditioning_key -> repeat: 0.000131s
INFO:root:Embedded Prompt: 0.024457s
INFO:root:Projector state: 0.000229s
INFO:root:Projector action: 0.000181s
INFO:root:[INFO] Apply model logic
INFO:root:[INFO] DDIMSampler.sample
INFO:root:prepare input of ddim_sampling: 0.022140s
INFO:root:prepare input of feeding: 0.006662s
INFO:root:[INFO] ========= Iteration 50 in total =========
INFO:root:[INFO] ========= Iteration 0/50 =========
INFO:root:[INFO] p_sample_ddim
INFO:root:[INFO] unconditional_conditioning
INFO:root:[INFO] apply_model
INFO:root:[INFO] apply model on WMAModel
INFO:root:timestep_embedding: 0.015853s
INFO:root:time_embed: 0.015716s
INFO:root:else on base_model_gen_only: 0.001398s
INFO:root:repeat_interleave: 0.000036s
INFO:root:rearrange_1: 0.000039s
INFO:root:fs_condition: 0.000402s
INFO:root:[INFO] Run on Iteration total12
INFO:root:[INFO] ========= 0/12 =========
INFO:root:module: 0.001341s
INFO:root:init_attn: 0.128801s
INFO:root:[INFO] ========= 1/12 =========
/var/spool/slurm/slurmd/job04200/slurm_script: line 66: 1851581 Segmentation fault      (core dumped) $PYTHON unifolm-world-model-action/scripts/evaluation/world_model_interaction.py --seed 123 --ckpt_path $ckpt --config $config --savedir "/mnt/$USER/run_workspace/result/force/testing/unitree_z1_stackbox" --bs 1 --height 320 --width 512 --unconditional_guidance_scale 1.0 --ddim_steps 50 --ddim_eta 1.0 --prompt_dir "./ASC26-Embodied-World-Model-Optimization/unitree_z1_stackbox/case1/world_model_interaction_prompts" --dataset "unitree_z1_stackbox" --video_length 16 --frame_stride 4 --n_action_steps 16 --exe_steps 16 --n_iter 12 --timestep_spacing 'uniform_trailing' --guidance_rescale 0.7 --perframe_ae
All jobs completed!
