Switching to group 'render' and re-executing...
True
Checking GPU Availability...
Torch version: 2.9.1+rocm7.1.1.git351ff442
Is ROCm/CUDA available?: True
Device count: 1
-------------------------------------------
Processing dataset: 
2026-02-17 05:36:00.802622: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-02-17 05:36:00.855824: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-02-17 05:36:00.860312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-02-17 05:36:00.867638: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-02-17 05:36:00.881660: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-02-17 05:36:01.508824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[rank: 0] Global seed set to 123
Nvidia GPU detected!
Total visible GPUs: 1
--- GPU 0 ---
Name: AMD Radeon PRO W7900D
Total memory: 49136 MB
it's find only 1 gpus can't do multi gpu
still run it's any ways
INFO:root:[INFO] Start Inference
INFO:root:init valueable: 0.005971s
/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
INFO:mainlogger:LatentVisualDiffusion: Running in v-prediction mode
Force INTO XFORMERS IS AVAILBLE
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
[INFO Using efficient_forward]
[INFO Using efficient_forward]
[INFO Using normal_forward]
[INFO Using normal_forward]
INFO:unifolm_wma_run.models.diffusion_head.conditional_unet1d:number of parameters: 5.010531e+08
INFO:unifolm_wma_run.models.diffusion_head.conditional_unet1d:number of parameters: 5.010531e+08
INFO:root:[INFO] this is the model architecture
INFO:root:[INFO] input_blocks
INFO:root:ModuleList(
  (0): TimestepEmbedSequential(
    (0): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (1-2): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=320, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=1024, out_features=320, bias=False)
            (to_v): Linear(in_features=1024, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=320, bias=False)
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
  )
  (3): TimestepEmbedSequential(
    (0): Downsample(
      (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (4): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (5): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (6): TimestepEmbedSequential(
    (0): Downsample(
      (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (7): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
  (8): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
  (9): TimestepEmbedSequential(
    (0): Downsample(
      (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (10-11): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
  )
)
INFO:root:[INFO] output_blocks
INFO:root:ModuleList(
  (0-1): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
  )
  (2): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): Upsample(
      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (3-4): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 2560, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
  (5): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v): Linear(in_features=1024, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=1280, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=1280, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=1280, bias=False)
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (3): Upsample(
      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (6): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1920, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (7): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 1280, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (8): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=640, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=1024, out_features=640, bias=False)
            (to_v): Linear(in_features=1024, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=640, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=640, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=640, bias=False)
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (3): Upsample(
      (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (9): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 960, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=320, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=1024, out_features=320, bias=False)
            (to_v): Linear(in_features=1024, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=320, bias=False)
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
  )
  (10-11): 2 x TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNormSpecific(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1280, out_features=320, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNormSpecific(32, 320, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      (temopral_conv): TemporalConvBlock(
        (conv1): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv2): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv3): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
        (conv4): Sequential(
          (0): GroupNorm(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))
        )
      )
    )
    (1): SpatialTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=1024, out_features=320, bias=False)
            (to_v): Linear(in_features=1024, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
            (to_k_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_ip): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_as): Linear(in_features=1024, out_features=320, bias=False)
            (to_k_aa): Linear(in_features=1024, out_features=320, bias=False)
            (to_v_aa): Linear(in_features=1024, out_features=320, bias=False)
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
    (2): TemporalTransformer(
      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=320, out_features=320, bias=True)
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=320, out_features=2560, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=1280, out_features=320, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=320, out_features=320, bias=False)
            (to_k): Linear(in_features=320, out_features=320, bias=False)
            (to_v): Linear(in_features=320, out_features=320, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=320, out_features=320, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
      )
      (proj_out): Linear(in_features=320, out_features=320, bias=True)
    )
  )
)
INFO:root:[INFO] action_unet
INFO:root:ConditionalUnet1D(
  (obs_encoder): MultiImageObsEncoder(
    (key_model_map): ModuleDict(
      (image): Sequential(
        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (1): GroupNorm(4, 64, eps=1e-05, affine=True)
        (2): ReLU(inplace=True)
        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (4): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)
          )
          (1): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)
          )
        )
        (5): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)
            (downsample): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): GroupNorm(8, 128, eps=1e-05, affine=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)
          )
        )
        (6): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)
            (downsample): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): GroupNorm(16, 256, eps=1e-05, affine=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)
          )
        )
        (7): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): GroupNorm(32, 512, eps=1e-05, affine=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)
          )
        )
      )
    )
    (key_transform_map): ModuleDict(
      (image): Sequential(
        (0): Identity()
        (1): Identity()
        (2): Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      )
    )
    (spatial_softmax): SpatialSoftmax(num_kp=128, temperature=1.0, noise=0.0)
  )
  (mid_modules): ModuleList(
    (0-1): 2 x ConditionalResidualBlock1D(
      (blocks): ModuleList(
        (0-1): 2 x Conv1dBlock(
          (block): Sequential(
            (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
            (2): Mish()
          )
        )
      )
      (cond_encoder): Sequential(
        (0): Mish()
        (1): Linear(in_features=3232, out_features=4096, bias=True)
      )
      (residual_conv): Identity()
    )
    (2): Identity()
  )
  (spatial_softmax_blocks): ModuleList(
    (0): SpatialSoftmax(num_kp=320, temperature=1.0, noise=0.0)
    (1): SpatialSoftmax(num_kp=640, temperature=1.0, noise=0.0)
    (2-6): 5 x SpatialSoftmax(num_kp=1280, temperature=1.0, noise=0.0)
    (7): SpatialSoftmax(num_kp=640, temperature=1.0, noise=0.0)
    (8): SpatialSoftmax(num_kp=320, temperature=1.0, noise=0.0)
  )
  (diffusion_step_encoder): Sequential(
    (0): SinusoidalPosEmb()
    (1): Linear(in_features=128, out_features=512, bias=True)
    (2): Mish()
    (3): Linear(in_features=512, out_features=128, bias=True)
  )
  (up_modules): ModuleList(
    (0): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(4096, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Conv1d(4096, 2048, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Upsample1d(
        (conv): ConvTranspose1d(2048, 2048, kernel_size=(4,), stride=(2,), padding=(1,))
      )
    )
    (1): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(3072, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Conv1d(3072, 1024, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Upsample1d(
        (conv): ConvTranspose1d(1024, 1024, kernel_size=(4,), stride=(2,), padding=(1,))
      )
    )
    (2): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1536, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Upsample1d(
        (conv): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))
      )
    )
    (3): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(768, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Identity()
      )
      (2-3): 2 x Identity()
    )
  )
  (down_modules): ModuleList(
    (0): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(16, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Conv1d(16, 256, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 256, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1312, out_features=512, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Downsample1d(
        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (1): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 512, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=1952, out_features=1024, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Downsample1d(
        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (2): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=2048, bias=True)
        )
        (residual_conv): Identity()
      )
      (2): Identity()
      (3): Downsample1d(
        (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))
      )
    )
    (3): ModuleList(
      (0): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(1024, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
          (1): Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))
      )
      (1): ConditionalResidualBlock1D(
        (blocks): ModuleList(
          (0-1): 2 x Conv1dBlock(
            (block): Sequential(
              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)
              (2): Mish()
            )
          )
        )
        (cond_encoder): Sequential(
          (0): Mish()
          (1): Linear(in_features=3232, out_features=4096, bias=True)
        )
        (residual_conv): Identity()
      )
      (2-3): 2 x Identity()
    )
  )
  (final_conv): Sequential(
    (0): Conv1dBlock(
      (block): Sequential(
        (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): GroupNorm(8, 256, eps=1e-05, affine=True)
        (2): Mish()
      )
    )
    (1): Conv1d(256, 16, kernel_size=(1,), stride=(1,))
  )
  (proj_in_action): Sequential(
    (0): Linear(in_features=1, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (proj_in_horizon): Sequential(
    (0): Linear(in_features=16, out_features=32, bias=True)
    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (proj_out_action): Sequential(
    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=32, out_features=1, bias=True)
  )
  (proj_out_horizon): Sequential(
    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=32, out_features=16, bias=True)
  )
)
INFO:root:[INFO] Done load WMAmodel
AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
INFO:root:Loaded ViT-H-14 model config.
INFO:root:Loaded ViT-H-14 model config.
/mnt/ASC1664/unifolm-wma-0-dual/ASC26-Embodied-World-Model-Optimization/ckpts/unifolm_wma_dual.ckpt
INFO:root:load models: 32.176049s
>>> model checkpoint loaded.
>>> Load pre-trained model ...
INFO:root:load model checkpoint: 19.417570s
INFO:root:***** Configing Data *****
>>> unitree_z1_stackbox: 1 data samples loaded.
>>> unitree_z1_stackbox: data stats loaded.
>>> unitree_z1_stackbox: normalizer initiated.
>>> unitree_z1_dual_arm_stackbox: 1 data samples loaded.
>>> unitree_z1_dual_arm_stackbox: data stats loaded.
>>> unitree_z1_dual_arm_stackbox: normalizer initiated.
>>> unitree_z1_dual_arm_stackbox_v2: 1 data samples loaded.
>>> unitree_z1_dual_arm_stackbox_v2: data stats loaded.
>>> unitree_z1_dual_arm_stackbox_v2: normalizer initiated.
>>> unitree_z1_dual_arm_cleanup_pencils: 1 data samples loaded.
>>> unitree_z1_dual_arm_cleanup_pencils: data stats loaded.
>>> unitree_z1_dual_arm_cleanup_pencils: normalizer initiated.
>>> unitree_g1_pack_camera: 1 data samples loaded.
>>> unitree_g1_pack_camera: data stats loaded.
>>> unitree_g1_pack_camera: normalizer initiated.
>>> Dataset is successfully loaded ...
INFO:root:load data: 0.060424s
>>> Generate 16 frames under each generation ...
INFO:root:set up data: 0.000027s
INFO:root:[INFO] Start running Inference
INFO:root:get_init_frame_path: 0.000046s
INFO:root:mkdir: 0.000109s
INFO:root:get_transition_path: 0.000016s
DEBUG:h5py._conv:Creating converter from 3 to 5
INFO:root:load file h5py: 0.007823s
INFO:root:[INFO] start running on loop frame_stride
INFO:root:mkdir => save dir: 0.000072s
INFO:root:init value: 0.000010s
DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13
DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 41 9
DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 62 4096
INFO:root:PIL open image : 0.015590s
INFO:root:create action & state: 0.000299s
INFO:root:normalize action - state: 0.000720s
INFO:root:spatial_transform: 0.001380s
INFO:root:prepare_init_input : 0.032147s
INFO:root:init observation: 0.006802s
INFO:root:init condition input w/ populate_queues: 0.000017s
INFO:root:[INFO] start running on loop n_iter
INFO:root:[INFO] start 1/12 n_iter (start with idx:1)
INFO:root:get decision model observation: 0.016896s
>>> Step 1: generating actions ...
INFO:root:[INFO] Step 1: generating actions ...
INFO:root:[INFO] IN image_guided_synthesis_sim_mode function
INFO:root:init model & noise: 0.000351s
INFO:root:permute image: 0.000013s
INFO:root:rearrange image: 0.001172s
/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/functional.py:6487: UserWarning: Flash Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:309.)
  attn_output = scaled_dot_product_attention(
/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/functional.py:6487: UserWarning: Mem Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:360.)
  attn_output = scaled_dot_product_attention(
INFO:root:CLIP Embedded Image: 0.740747s
INFO:root:Projector Image model: 0.004225s
INFO:root:[INFO] get_latent_z function
INFO:root:get_latent_z => rearrange video: 0.000115s
INFO:root:get_latent_z => encode_first_stage: 0.130830s
INFO:root:get_latent_z => rearrange z: 0.000121s
INFO:root:conditioning_key -> repeat: 0.000129s
INFO:root:Embedded Prompt: 0.016379s
INFO:root:Projector state: 0.000252s
INFO:root:Projector action: 0.000179s
INFO:root:[INFO] Apply model logic
INFO:root:[INFO] DDIMSampler.sample
INFO:root:prepare input of ddim_sampling: 0.020672s
INFO:root:prepare input of feeding: 0.006517s
INFO:root:[INFO] ========= Iteration 50 in total =========
INFO:root:[INFO] ========= Iteration 0/50 =========
INFO:root:[INFO] p_sample_ddim
INFO:root:[INFO] unconditional_conditioning
INFO:root:[INFO] apply_model
INFO:root:[INFO] apply model on WMAModel
INFO:root:timestep_embedding: 0.016608s
INFO:root:time_embed: 0.016079s
INFO:root:else on base_model_gen_only: 0.001379s
INFO:root:repeat_interleave: 0.000038s
INFO:root:rearrange_1: 0.000033s
INFO:root:fs_condition: 0.031525s
INFO:root:[INFO] Run on Iteration total12
INFO:root:[INFO] ========= 0/12 =========
INFO:root:module: 0.001415s
INFO:root:init_attn: 0.165732s
INFO:root:[INFO] ========= 1/12 =========
Q: torch.Size([80, 2560, 64])
K: torch.Size([80, 2560, 64])
K: torch.Size([80, 2560, 64])
Q: torch.Size([80, 2560, 64])
K: torch.Size([80, 77, 64])
K: torch.Size([80, 77, 64])
INFO:root:module: 0.060173s
Traceback (most recent call last):
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/scripts/evaluation/world_model_interaction.py", line 1002, in <module>
    run_inference(args, gpu_num, rank, "None")
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/scripts/evaluation/world_model_interaction.py", line 714, in run_inference
    pred_videos_0, pred_actions, _ = image_guided_synthesis_sim_mode(
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/scripts/evaluation/world_model_interaction.py", line 471, in image_guided_synthesis_sim_mode
    samples, actions, states, intermedia = ddim_sampler.sample(
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/models/samplers/ddim.py", line 177, in sample
    samples, actions, states, intermediates = self.ddim_sampling(
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/models/samplers/ddim.py", line 303, in ddim_sampling
    outs = self.p_sample_ddim(
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/models/samplers/ddim.py", line 393, in p_sample_ddim
    model_output, model_output_action, model_output_state = self.model.apply_model(
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/models/ddpms.py", line 1314, in apply_model
    x_recon, x_action_recon, x_state_recon = self.model(
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/models/ddpms.py", line 2522, in forward
    out = self.diffusion_model(xc,
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/modules/networks/wma_model.py", line 843, in forward
    h = module(h, emb, context=context, batch_size=b) ##TODO 
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/modules/networks/wma_model.py", line 60, in forward
    x = layer(x, context)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/modules/attention.py", line 680, in forward
    x = block(x, context=context, **kwargs)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/modules/attention.py", line 588, in forward
    return checkpoint(self._forward, input_tuple, self.parameters(),
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/utils/common.py", line 111, in checkpoint
    return func(*inputs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/modules/attention.py", line 595, in _forward
    x = self.attn2(self.norm2(x), context=context, mask=mask) + x
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/ASC1664/miniconda3/envs/unifolm-wma/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/modules/attention.py", line 409, in efficient_forward
    out = naive_attention(q, k, v, attn_bias=None) #this line 
  File "/mnt/ASC1664/run_workspace/unifolm-world-model-action/src/unifolm_wma_run/modules/attention.py", line 106, in naive_attention
    attn = query @ key.transpose(-2, -1)  # (B,H,N,N)
RuntimeError: Expected size for first two dimensions of batch2 tensor to be: [80, 2560] but got: [80, 77].
All jobs completed!
