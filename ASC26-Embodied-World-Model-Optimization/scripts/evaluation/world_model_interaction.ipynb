{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513ff64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\n",
    "    \"/mnt/ASC1664/unifolm-wma-0-dual/ASC26-Embodied-World-Model-Optimization/scripts/evaluation\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9548e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"world_model_interaction.py\",\n",
    "    \"--seed\", \"123\",\n",
    "    \"--ckpt_path\", \"ckpts/unifolm_wma_dual.ckpt\",\n",
    "    \"--config\", \"configs/inference/world_model_interaction.yaml\",\n",
    "    \"--savedir\", \"unitree_z1_dual_arm_cleanup_pencils/case1/output\",\n",
    "    \"--bs\", \"1\",\n",
    "    \"--height\", \"320\",\n",
    "    \"--width\", \"512\",\n",
    "    \"--unconditional_guidance_scale\", \"1.0\",\n",
    "    \"--ddim_steps\", \"50\",\n",
    "    \"--ddim_eta\", \"1.0\",\n",
    "    \"--prompt_dir\", \"unitree_z1_dual_arm_cleanup_pencils/case1/world_model_interaction_prompts\",\n",
    "    \"--dataset\", \"unitree_z1_dual_arm_cleanup_pencils\",\n",
    "    \"--video_length\", \"16\",\n",
    "    \"--frame_stride\", \"4\",          # ⭐ ตัวที่ขาด\n",
    "    \"--n_action_steps\", \"16\",\n",
    "    \"--exe_steps\", \"16\",\n",
    "    \"--n_iter\", \"8\",\n",
    "    \"--timestep_spacing\", \"uniform_trailing\",\n",
    "    \"--guidance_rescale\", \"0.7\",\n",
    "    \"--perframe_ae\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a1a3377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:mainlogger:LatentVisualDiffusion: Running in v-prediction mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:unifolm_wma.models.diffusion_head.conditional_unet1d:number of parameters: 5.010531e+08\n",
      "INFO:unifolm_wma.models.diffusion_head.conditional_unet1d:number of parameters: 5.010531e+08\n",
      "AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "INFO:root:Loaded ViT-H-14 model config.\n",
      "INFO:root:Loaded ViT-H-14 model config.\n",
      ">>> model checkpoint loaded.\n",
      ">>> Load pre-trained model ...\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2: standard forward\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2: efficient_forward (xformers)\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1: standard forward\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2: standard forward\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import h5py\n",
    "import numpy as np\n",
    "import logging\n",
    "import einops\n",
    "import warnings\n",
    "import imageio\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from eval_utils import populate_queues, log_to_tensorboard\n",
    "from collections import deque\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "\n",
    "from unifolm_wma.models.samplers.ddim import DDIMSampler\n",
    "from unifolm_wma.utils.utils import instantiate_from_config\n",
    "\n",
    "\n",
    "def get_device_from_parameters(module: nn.Module) -> torch.device:\n",
    "    \"\"\"Get a module's device by checking one of its parameters.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): The model whose device is to be inferred.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The device of the model's parameters.\n",
    "    \"\"\"\n",
    "    return next(iter(module.parameters())).device\n",
    "\n",
    "\n",
    "def write_video(video_path: str, stacked_frames: list, fps: int) -> None:\n",
    "    \"\"\"Save a list of frames to a video file.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Output path for the video.\n",
    "        stacked_frames (list): List of image frames.\n",
    "        fps (int): Frames per second for the video.\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\",\n",
    "                                \"pkg_resources is deprecated as an API\",\n",
    "                                category=DeprecationWarning)\n",
    "        imageio.mimsave(video_path, stacked_frames, fps=fps)\n",
    "\n",
    "\n",
    "def get_filelist(data_dir: str, postfixes: list[str]) -> list[str]:\n",
    "    \"\"\"Return sorted list of files in a directory matching specified postfixes.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory path to search in.\n",
    "        postfixes (list[str]): List of file extensions to match.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Sorted list of file paths.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        os.path.join(data_dir, f\"*.{postfix}\") for postfix in postfixes\n",
    "    ]\n",
    "    file_list = []\n",
    "    for pattern in patterns:\n",
    "        file_list.extend(glob.glob(pattern))\n",
    "    file_list.sort()\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model: nn.Module, ckpt: str) -> nn.Module:\n",
    "    \"\"\"Load model weights from checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model instance.\n",
    "        ckpt (str): Path to the checkpoint file.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Model with loaded weights.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"state_dict\" in list(state_dict.keys()):\n",
    "        state_dict = state_dict[\"state_dict\"]\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        except:\n",
    "            new_pl_sd = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                new_pl_sd[k] = v\n",
    "\n",
    "            for k in list(new_pl_sd.keys()):\n",
    "                if \"framestride_embed\" in k:\n",
    "                    new_key = k.replace(\"framestride_embed\", \"fps_embedding\")\n",
    "                    new_pl_sd[new_key] = new_pl_sd[k]\n",
    "                    del new_pl_sd[k]\n",
    "            model.load_state_dict(new_pl_sd, strict=True)\n",
    "    else:\n",
    "        new_pl_sd = OrderedDict()\n",
    "        for key in state_dict['module'].keys():\n",
    "            new_pl_sd[key[16:]] = state_dict['module'][key]\n",
    "        model.load_state_dict(new_pl_sd)\n",
    "    print('>>> model checkpoint loaded.')\n",
    "    return model\n",
    "\n",
    "\n",
    "def is_inferenced(save_dir: str, filename: str) -> bool:\n",
    "    \"\"\"Check if a given filename has already been processed and saved.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): Directory where results are saved.\n",
    "        filename (str): Name of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if processed file exists, False otherwise.\n",
    "    \"\"\"\n",
    "    video_file = os.path.join(save_dir, \"samples_separate\",\n",
    "                              f\"{filename[:-4]}_sample0.mp4\")\n",
    "    return os.path.exists(video_file)\n",
    "\n",
    "\n",
    "def save_results(video: Tensor, filename: str, fps: int = 8) -> None:\n",
    "    \"\"\"Save video tensor to file using torchvision.\n",
    "\n",
    "    Args:\n",
    "        video (Tensor): Tensor of shape (B, C, T, H, W).\n",
    "        filename (str): Output file path.\n",
    "        fps (int, optional): Frames per second. Defaults to 8.\n",
    "    \"\"\"\n",
    "    video = video.detach().cpu()\n",
    "    video = torch.clamp(video.float(), -1., 1.)\n",
    "    n = video.shape[0]\n",
    "    video = video.permute(2, 0, 1, 3, 4)\n",
    "\n",
    "    frame_grids = [\n",
    "        torchvision.utils.make_grid(framesheet, nrow=int(n), padding=0)\n",
    "        for framesheet in video\n",
    "    ]\n",
    "    grid = torch.stack(frame_grids, dim=0)\n",
    "    grid = (grid + 1.0) / 2.0\n",
    "    grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1)\n",
    "    torchvision.io.write_video(filename,\n",
    "                               grid,\n",
    "                               fps=fps,\n",
    "                               video_codec='h264',\n",
    "                               options={'crf': '10'})\n",
    "\n",
    "\n",
    "def get_init_frame_path(data_dir: str, sample: dict) -> str:\n",
    "    \"\"\"Construct the init_frame path from directory and sample metadata.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Base directory containing videos.\n",
    "        sample (dict): Dictionary containing 'data_dir' and 'videoid'.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the video file.\n",
    "    \"\"\"\n",
    "    rel_video_fp = os.path.join(sample['data_dir'],\n",
    "                                str(sample['videoid']) + '.png')\n",
    "    full_image_fp = os.path.join(data_dir, 'images', rel_video_fp)\n",
    "    return full_image_fp\n",
    "\n",
    "\n",
    "def get_transition_path(data_dir: str, sample: dict) -> str:\n",
    "    \"\"\"Construct the full transition file path from directory and sample metadata.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Base directory containing transition files.\n",
    "        sample (dict): Dictionary containing 'data_dir' and 'videoid'.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the HDF5 transition file.\n",
    "    \"\"\"\n",
    "    rel_transition_fp = os.path.join(sample['data_dir'],\n",
    "                                     str(sample['videoid']) + '.h5')\n",
    "    full_transition_fp = os.path.join(data_dir, 'transitions',\n",
    "                                      rel_transition_fp)\n",
    "    return full_transition_fp\n",
    "\n",
    "\n",
    "def prepare_init_input(start_idx: int,\n",
    "                       init_frame_path: str,\n",
    "                       transition_dict: dict[str, torch.Tensor],\n",
    "                       frame_stride: int,\n",
    "                       wma_data,\n",
    "                       video_length: int = 16,\n",
    "                       n_obs_steps: int = 2) -> dict[str, Tensor]:\n",
    "    \"\"\"\n",
    "    Extracts a structured sample from a video sequence including frames, states, and actions,\n",
    "    along with properly padded observations and pre-processed tensors for model input.\n",
    "\n",
    "    Args:\n",
    "        start_idx (int): Starting frame index for the current clip.\n",
    "        video: decord video instance.\n",
    "        transition_dict (Dict[str, Tensor]): Dictionary containing tensors for 'action', \n",
    "                                             'observation.state', 'action_type', 'state_type'.\n",
    "        frame_stride (int): Temporal stride between sampled frames.\n",
    "        wma_data: Object that holds configuration and utility functions like normalization, \n",
    "                transformation, and resolution info.\n",
    "        video_length (int, optional): Number of frames to sample from the video. Default is 16.\n",
    "        n_obs_steps (int, optional): Number of historical steps for observations. Default is 2.\n",
    "    \"\"\"\n",
    "\n",
    "    indices = [start_idx + frame_stride * i for i in range(video_length)]\n",
    "    init_frame = Image.open(init_frame_path).convert('RGB')\n",
    "    init_frame = torch.tensor(np.array(init_frame)).unsqueeze(0).permute(\n",
    "        3, 0, 1, 2).float()\n",
    "\n",
    "    if start_idx < n_obs_steps - 1:\n",
    "        state_indices = list(range(0, start_idx + 1))\n",
    "        states = transition_dict['observation.state'][state_indices, :]\n",
    "        num_padding = n_obs_steps - 1 - start_idx\n",
    "        first_slice = states[0:1, :]  # (t, d)\n",
    "        padding = first_slice.repeat(num_padding, 1)\n",
    "        states = torch.cat((padding, states), dim=0)\n",
    "    else:\n",
    "        state_indices = list(range(start_idx - n_obs_steps + 1, start_idx + 1))\n",
    "        states = transition_dict['observation.state'][state_indices, :]\n",
    "\n",
    "    actions = transition_dict['action'][indices, :]\n",
    "\n",
    "    ori_state_dim = states.shape[-1]\n",
    "    ori_action_dim = actions.shape[-1]\n",
    "\n",
    "    frames_action_state_dict = {\n",
    "        'action': actions,\n",
    "        'observation.state': states,\n",
    "    }\n",
    "    frames_action_state_dict = wma_data.normalizer(frames_action_state_dict)\n",
    "    frames_action_state_dict = wma_data.get_uni_vec(\n",
    "        frames_action_state_dict,\n",
    "        transition_dict['action_type'],\n",
    "        transition_dict['state_type'],\n",
    "    )\n",
    "\n",
    "    if wma_data.spatial_transform is not None:\n",
    "        init_frame = wma_data.spatial_transform(init_frame)\n",
    "    init_frame = (init_frame / 255 - 0.5) * 2\n",
    "\n",
    "    data = {\n",
    "        'observation.image': init_frame,\n",
    "    }\n",
    "    data.update(frames_action_state_dict)\n",
    "    return data, ori_state_dim, ori_action_dim\n",
    "\n",
    "\n",
    "def get_latent_z(model, videos: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Extracts latent features from a video batch using the model's first-stage encoder.\n",
    "\n",
    "    Args:\n",
    "        model: the world model.\n",
    "        videos (Tensor): Input videos of shape [B, C, T, H, W].\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Latent video tensor of shape [B, C, T, H, W].\n",
    "    \"\"\"\n",
    "    b, c, t, h, w = videos.shape\n",
    "    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n",
    "    z = model.encode_first_stage(x)\n",
    "    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)\n",
    "    return z\n",
    "\n",
    "\n",
    "def preprocess_observation(\n",
    "        model, observations: dict[str, np.ndarray]) -> dict[str, Tensor]:\n",
    "    \"\"\"Convert environment observation to LeRobot format observation.\n",
    "    Args:\n",
    "        observation: Dictionary of observation batches from a Gym vector environment.\n",
    "    Returns:\n",
    "        Dictionary of observation batches with keys renamed to LeRobot format and values as tensors.\n",
    "    \"\"\"\n",
    "    # Map to expected inputs for the policy\n",
    "    return_observations = {}\n",
    "\n",
    "    if isinstance(observations[\"pixels\"], dict):\n",
    "        imgs = {\n",
    "            f\"observation.images.{key}\": img\n",
    "            for key, img in observations[\"pixels\"].items()\n",
    "        }\n",
    "    else:\n",
    "        imgs = {\"observation.images.top\": observations[\"pixels\"]}\n",
    "\n",
    "    for imgkey, img in imgs.items():\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        # Sanity check that images are channel last\n",
    "        _, h, w, c = img.shape\n",
    "        assert c < h and c < w, f\"expect channel first images, but instead {img.shape}\"\n",
    "\n",
    "        # Sanity check that images are uint8\n",
    "        assert img.dtype == torch.uint8, f\"expect torch.uint8, but instead {img.dtype=}\"\n",
    "\n",
    "        # Convert to channel first of type float32 in range [0,1]\n",
    "        img = einops.rearrange(img, \"b h w c -> b c h w\").contiguous()\n",
    "        img = img.type(torch.float32)\n",
    "\n",
    "        return_observations[imgkey] = img\n",
    "\n",
    "    return_observations[\"observation.state\"] = torch.from_numpy(\n",
    "        observations[\"agent_pos\"]).float()\n",
    "    return_observations['observation.state'] = model.normalize_inputs({\n",
    "        'observation.state':\n",
    "        return_observations['observation.state'].to(model.device)\n",
    "    })['observation.state']\n",
    "\n",
    "    return return_observations\n",
    "\n",
    "\n",
    "def image_guided_synthesis_sim_mode(\n",
    "        model: torch.nn.Module,\n",
    "        prompts: list[str],\n",
    "        observation: dict,\n",
    "        noise_shape: tuple[int, int, int, int, int],\n",
    "        action_cond_step: int = 16,\n",
    "        n_samples: int = 1,\n",
    "        ddim_steps: int = 50,\n",
    "        ddim_eta: float = 1.0,\n",
    "        unconditional_guidance_scale: float = 1.0,\n",
    "        fs: int | None = None,\n",
    "        text_input: bool = True,\n",
    "        timestep_spacing: str = 'uniform',\n",
    "        guidance_rescale: float = 0.0,\n",
    "        sim_mode: bool = True,\n",
    "        **kwargs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Performs image-guided video generation in a simulation-style mode with optional multimodal guidance (image, state, action, text).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The diffusion-based generative model with multimodal conditioning.\n",
    "        prompts (list[str]): A list of textual prompts to guide the synthesis process.\n",
    "        observation (dict): A dictionary containing observed inputs including:\n",
    "            - 'observation.images.top': Tensor of shape [B, O, C, H, W] (top-down images)\n",
    "            - 'observation.state': Tensor of shape [B, O, D] (state vector)\n",
    "            - 'action': Tensor of shape [B, T, D] (action sequence)\n",
    "        noise_shape (tuple[int, int, int, int, int]): Shape of the latent variable to generate, \n",
    "            typically (B, C, T, H, W).\n",
    "        action_cond_step (int): Number of time steps where action conditioning is applied. Default is 16.\n",
    "        n_samples (int): Number of samples to generate (unused here, always generates 1). Default is 1.\n",
    "        ddim_steps (int): Number of DDIM sampling steps. Default is 50.\n",
    "        ddim_eta (float): DDIM eta parameter controlling the stochasticity. Default is 1.0.\n",
    "        unconditional_guidance_scale (float): Scale for classifier-free guidance. If 1.0, guidance is off.\n",
    "        fs (int | None): Frame index to condition on, broadcasted across the batch if specified. Default is None.\n",
    "        text_input (bool): Whether to use text prompt as conditioning. If False, uses empty strings. Default is True.\n",
    "        timestep_spacing (str): Timestep sampling method in DDIM sampler. Typically \"uniform\" or \"linspace\".\n",
    "        guidance_rescale (float): Guidance rescaling factor to mitigate overexposure from classifier-free guidance.\n",
    "        sim_mode (bool): Whether to perform world-model interaction or decision-making using the world-model.\n",
    "        **kwargs: Additional arguments passed to the DDIM sampler.\n",
    "\n",
    "    Returns:\n",
    "        batch_variants (torch.Tensor): Predicted pixel-space video frames [B, C, T, H, W].\n",
    "        actions (torch.Tensor): Predicted action sequences [B, T, D] from diffusion decoding.\n",
    "        states (torch.Tensor): Predicted state sequences [B, T, D] from diffusion decoding.\n",
    "    \"\"\"\n",
    "    b, _, t, _, _ = noise_shape\n",
    "    ddim_sampler = DDIMSampler(model)\n",
    "    batch_size = noise_shape[0]\n",
    "\n",
    "    fs = torch.tensor([fs] * batch_size, dtype=torch.long, device=model.device)\n",
    "\n",
    "    img = observation['observation.images.top'].permute(0, 2, 1, 3, 4)\n",
    "    cond_img = rearrange(img, 'b o c h w -> (b o) c h w')[-1:]\n",
    "    cond_img_emb = model.embedder(cond_img)\n",
    "    cond_img_emb = model.image_proj_model(cond_img_emb)\n",
    "\n",
    "    if model.model.conditioning_key == 'hybrid':\n",
    "        z = get_latent_z(model, img.permute(0, 2, 1, 3, 4))\n",
    "        img_cat_cond = z[:, :, -1:, :, :]\n",
    "        img_cat_cond = repeat(img_cat_cond,\n",
    "                              'b c t h w -> b c (repeat t) h w',\n",
    "                              repeat=noise_shape[2])\n",
    "        cond = {\"c_concat\": [img_cat_cond]}\n",
    "\n",
    "    if not text_input:\n",
    "        prompts = [\"\"] * batch_size\n",
    "    cond_ins_emb = model.get_learned_conditioning(prompts)\n",
    "\n",
    "    cond_state_emb = model.state_projector(observation['observation.state'])\n",
    "    cond_state_emb = cond_state_emb + model.agent_state_pos_emb\n",
    "\n",
    "    cond_action_emb = model.action_projector(observation['action'])\n",
    "    cond_action_emb = cond_action_emb + model.agent_action_pos_emb\n",
    "\n",
    "    if not sim_mode:\n",
    "        cond_action_emb = torch.zeros_like(cond_action_emb)\n",
    "\n",
    "    cond[\"c_crossattn\"] = [\n",
    "        torch.cat(\n",
    "            [cond_state_emb, cond_action_emb, cond_ins_emb, cond_img_emb],\n",
    "            dim=1)\n",
    "    ]\n",
    "    cond[\"c_crossattn_action\"] = [\n",
    "        observation['observation.images.top'][:, :,\n",
    "                                              -model.n_obs_steps_acting:],\n",
    "        observation['observation.state'][:, -model.n_obs_steps_acting:],\n",
    "        sim_mode,\n",
    "        False,\n",
    "    ]\n",
    "\n",
    "    uc = None\n",
    "    kwargs.update({\"unconditional_conditioning_img_nonetext\": None})\n",
    "    cond_mask = None\n",
    "    cond_z0 = None\n",
    "    if ddim_sampler is not None:\n",
    "        samples, actions, states, intermedia = ddim_sampler.sample(\n",
    "            S=ddim_steps,\n",
    "            conditioning=cond,\n",
    "            batch_size=batch_size,\n",
    "            shape=noise_shape[1:],\n",
    "            verbose=False,\n",
    "            unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "            unconditional_conditioning=uc,\n",
    "            eta=ddim_eta,\n",
    "            cfg_img=None,\n",
    "            mask=cond_mask,\n",
    "            x0=cond_z0,\n",
    "            fs=fs,\n",
    "            timestep_spacing=timestep_spacing,\n",
    "            guidance_rescale=guidance_rescale,\n",
    "            **kwargs)\n",
    "\n",
    "        # Reconstruct from latent to pixel space\n",
    "        batch_images = model.decode_first_stage(samples)\n",
    "        batch_variants = batch_images\n",
    "\n",
    "    return batch_variants, actions, states\n",
    "\n",
    "\n",
    "def run_inference(args: argparse.Namespace, gpu_num: int, gpu_no: int) -> None:\n",
    "    \"\"\"\n",
    "    Run inference pipeline on prompts and image inputs.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): Parsed command-line arguments.\n",
    "        gpu_num (int): Number of GPUs.\n",
    "        gpu_no (int): Index of the current GPU.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create inference and tensorboard dirs\n",
    "    os.makedirs(args.savedir + '/inference', exist_ok=True)\n",
    "    log_dir = args.savedir + f\"/tensorboard\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Load prompt\n",
    "    os.chdir(\"/mnt/ASC1664/unifolm-wma-0-dual/ASC26-Embodied-World-Model-Optimization\")\n",
    "\n",
    "    csv_path = os.path.join(args.prompt_dir, f\"{args.dataset}.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Load config\n",
    "    config = OmegaConf.load(args.config)\n",
    "    config['model']['params']['wma_config']['params'][\n",
    "        'use_checkpoint'] = False\n",
    "    model = instantiate_from_config(config.model)\n",
    "    model.perframe_ae = args.perframe_ae\n",
    "    assert os.path.exists(args.ckpt_path), \"Error: checkpoint Not Found!\"\n",
    "    model = load_model_checkpoint(model, args.ckpt_path)\n",
    "    model.eval()\n",
    "    print(f'>>> Load pre-trained model ...')\n",
    "\n",
    "    print(\"===== LEAF MODULES ONLY =====\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:\n",
    "            print(name, \"->\", module.__class__.__name__)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--savedir\",\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help=\"Path to save the results.\")\n",
    "    parser.add_argument(\"--ckpt_path\",\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help=\"Path to the model checkpoint.\")\n",
    "    parser.add_argument(\"--config\",\n",
    "                        type=str,\n",
    "                        help=\"Path to the model checkpoint.\")\n",
    "    parser.add_argument(\n",
    "        \"--prompt_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Directory containing videos and corresponding prompts.\")\n",
    "    parser.add_argument(\"--dataset\",\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help=\"the name of dataset to test\")\n",
    "    parser.add_argument(\n",
    "        \"--ddim_steps\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Number of DDIM steps. If non-positive, DDPM is used instead.\")\n",
    "    parser.add_argument(\n",
    "        \"--ddim_eta\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Eta for DDIM sampling. Set to 0.0 for deterministic results.\")\n",
    "    parser.add_argument(\"--bs\",\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"Batch size for inference. Must be 1.\")\n",
    "    parser.add_argument(\"--height\",\n",
    "                        type=int,\n",
    "                        default=320,\n",
    "                        help=\"Height of the generated images in pixels.\")\n",
    "    parser.add_argument(\"--width\",\n",
    "                        type=int,\n",
    "                        default=512,\n",
    "                        help=\"Width of the generated images in pixels.\")\n",
    "    parser.add_argument(\n",
    "        \"--frame_stride\",\n",
    "        type=int,\n",
    "        nargs='+',\n",
    "        required=True,\n",
    "        help=\n",
    "        \"frame stride control for 256 model (larger->larger motion), FPS control for 512 or 1024 model (smaller->larger motion)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--unconditional_guidance_scale\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Scale for classifier-free guidance during sampling.\")\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        type=int,\n",
    "                        default=123,\n",
    "                        help=\"Random seed for reproducibility.\")\n",
    "    parser.add_argument(\"--video_length\",\n",
    "                        type=int,\n",
    "                        default=16,\n",
    "                        help=\"Number of frames in the generated video.\")\n",
    "    parser.add_argument(\"--num_generation\",\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"seed for seed_everything\")\n",
    "    parser.add_argument(\n",
    "        \"--timestep_spacing\",\n",
    "        type=str,\n",
    "        default=\"uniform\",\n",
    "        help=\n",
    "        \"Strategy for timestep scaling. See Table 2 in the paper: 'Common Diffusion Noise Schedules and Sample Steps are Flawed' (https://huggingface.co/papers/2305.08891).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--guidance_rescale\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\n",
    "        \"Rescale factor for guidance as discussed in 'Common Diffusion Noise Schedules and Sample Steps are Flawed' (https://huggingface.co/papers/2305.08891).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--perframe_ae\",\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help=\n",
    "        \"Use per-frame autoencoder decoding to reduce GPU memory usage. Recommended for models with resolutions like 576x1024.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_action_steps\",\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help=\"num of samples per prompt\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--exe_steps\",\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help=\"num of samples to execute\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_iter\",\n",
    "        type=int,\n",
    "        default=40,\n",
    "        help=\"num of iteration to interact with the world model\",\n",
    "    )\n",
    "    parser.add_argument(\"--zero_pred_state\",\n",
    "                        action='store_true',\n",
    "                        default=False,\n",
    "                        help=\"not using the predicted states as comparison\")\n",
    "    parser.add_argument(\"--save_fps\",\n",
    "                        type=int,\n",
    "                        default=8,\n",
    "                        help=\"fps for the saving video\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    seed = args.seed\n",
    "    if seed < 0:\n",
    "        seed = random.randint(0, 2**31)\n",
    "    seed_everything(seed)\n",
    "    rank, gpu_num = 0, 1\n",
    "    run_inference(args, gpu_num, rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885f734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
